{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjAas5pGE9di",
        "outputId": "cbb3f096-d542-497e-f9f4-f00b7ac85caa"
      },
      "outputs": [],
      "source": [
        "# !pip install requests\n",
        "# !pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYiEzkfJFJWj"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lIztt9B2uok"
      },
      "source": [
        "## Le Parisien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD826-taFQI6"
      },
      "outputs": [],
      "source": [
        "def get_page_contents(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    page = requests.get(url, headers=headers)\n",
        "\n",
        "    if page.status_code == 200:\n",
        "        return page.text\n",
        "\n",
        "    return None\n",
        "\n",
        "def get_quotes_and_authors(page_contents):\n",
        "  soup = BeautifulSoup(page_contents, 'html.parser')\n",
        "\n",
        "  script_tag = soup.find('script', type='application/ld+json')\n",
        "\n",
        "  if script_tag:\n",
        "      json_data = json.loads(script_tag.string)\n",
        "      authors = soup.find_all('span', class_='author')\n",
        "      quotes = soup.find_all('section', class_='content')\n",
        "\n",
        "      return quotes, authors\n",
        "\n",
        "  return [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMg9d4DHgqdt",
        "outputId": "5027e373-2edc-4aca-aea7-75b1841ab66c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/Le Parisien.csv'\n",
        "# data = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ykyk41b9eVO"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def process_urls_from_csv(file_path, output_file):\n",
        "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        headers = next(reader, None)\n",
        "\n",
        "        with open(output_file, mode='w', newline='', encoding='utf-8') as output_csv:\n",
        "            csv_writer = csv.writer(output_csv)\n",
        "\n",
        "            csv_writer.writerow(['URL', 'Author', 'Quote'])\n",
        "\n",
        "            for row in reader:\n",
        "                for url in row:\n",
        "                    if url:\n",
        "                        page_contents = get_page_contents(url)\n",
        "                        if page_contents:\n",
        "                            quotes, author = get_quotes_and_authors(page_contents)\n",
        "                            print(f\"Processing URL: {url}\")\n",
        "                            print(f\"Author: {author.text}\")\n",
        "                            print(f\"Found {len(quotes)} quotes\")\n",
        "\n",
        "                            for quote in quotes:\n",
        "                                csv_writer.writerow([url, author.text, quote])\n",
        "                                print(f\"Wrote quote to CSV: {quote}\")\n",
        "                        else:\n",
        "                            print(f\"Failed to get contents for {url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "bfAFPZV3NPJw",
        "outputId": "ae6d4c00-e242-4a62-cad2-6433a5ea517e"
      },
      "outputs": [],
      "source": [
        "process_urls_from_csv(file_path, 'quotes_output.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Zplx_T3WPf"
      },
      "source": [
        "## Journal du Cameroun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVraCA6LEE1P"
      },
      "outputs": [],
      "source": [
        "def get_page_contents(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    page = requests.get(url, headers=headers)\n",
        "\n",
        "    if page.status_code == 200:\n",
        "        return page.text\n",
        "\n",
        "    return None\n",
        "\n",
        "def get_quotes_and_authors(page_contents):\n",
        "  soup = BeautifulSoup(page_contents, 'html.parser')\n",
        "\n",
        "  script_tag = soup.find('script', type='application/ld+json')\n",
        "\n",
        "  if script_tag:\n",
        "      json_data = json.loads(script_tag.string)\n",
        "      author_tag = soup.find('div', class_='post-meta-author')\n",
        "      authors = author_tag.find('a').get_text(strip=True) if author_tag else \"Author not found\"\n",
        "      article_content = soup.find('div', class_='article-content')\n",
        "      quotes = []\n",
        "      if article_content:\n",
        "          quotes += [p.text for p in article_content.find_all('p') if p.get_text(strip=True)]\n",
        "          quotes += [div.text for div in article_content.find_all('div') if div.get_text(strip=True)]\n",
        "\n",
        "      return quotes, authors\n",
        "\n",
        "  return [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zF26cF_Be10"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def process_urls_from_csv(file_path, output_file):\n",
        "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        headers = next(reader, None)  # Skip the header row if present\n",
        "\n",
        "        # Open output CSV file for writing\n",
        "        with open(output_file, mode='w', newline='', encoding='utf-8') as output_csv:\n",
        "            csv_writer = csv.writer(output_csv)\n",
        "\n",
        "            # Write headers to the output file\n",
        "            csv_writer.writerow(['URL', 'Author', 'Quote'])\n",
        "\n",
        "            for row in reader:\n",
        "                for url in row:\n",
        "                    if url:\n",
        "                        page_contents = get_page_contents(url)\n",
        "                        if page_contents:\n",
        "                            quotes, author = get_quotes_and_authors(page_contents)\n",
        "                            print(f\"Processing URL: {url}\")\n",
        "                            print(f\"Author: {author}\")\n",
        "                            print(f\"Found {len(quotes)} quotes\")\n",
        "\n",
        "                            for quote in quotes:\n",
        "                                # Write the data to the CSV\n",
        "                                csv_writer.writerow([url, author, quote])\n",
        "                                print(f\"Wrote quote to CSV: {quote}\")\n",
        "                        else:\n",
        "                            print(f\"Failed to get contents for {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqRan8byCsll",
        "outputId": "0a27a50f-b3aa-40ea-af99-2973f0f6d767"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Thesis/data/Journal du Cameroun.csv'\n",
        "process_urls_from_csv(file_path, 'cameroun_quotes_output.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y-grMSq3GbD"
      },
      "source": [
        "### Words per csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LWdZmyXZDxU",
        "outputId": "c9b7b0cb-11b8-4900-9fb6-e2d57edc3b7f"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/cameroun_quotes_output.csv\")\n",
        "# print(data.head())\n",
        "\n",
        "data[\"Number of Words\"] = data[\"Quote\"].apply(lambda n: len(n.split()))\n",
        "# print(data.head())\n",
        "\n",
        "total_words = data[\"Number of Words\"].sum()\n",
        "print(\"Total number of words in all quotes:\", total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-jK-VaRZyti",
        "outputId": "8dfe6c7a-b5bd-4c05-f451-2c5ac5f23a16"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/quotes_output.csv\")\n",
        "# print(data.head())\n",
        "\n",
        "data[\"Number of Words\"] = data[\"Quote\"].apply(lambda n: len(str(n).split()) if pd.notnull(n) else 0)\n",
        "# print(data.head())\n",
        "\n",
        "total_words = data[\"Number of Words\"].sum()\n",
        "print(\"Total number of words in all quotes:\", total_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbas-AA8jOml"
      },
      "source": [
        "# Replace Country Names with Placeholder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFh8CoNJjNmd",
        "outputId": "3c3572de-ec59-49cc-be73-e8d5cd62b6b5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def replace_country_nationality(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"\\b[Cc]ameroun\\b\", \"[COUNTRY]\", text)\n",
        "    text = re.sub(r\"\\b[Ff]rance\\b\", \"[COUNTRY]\", text)\n",
        "    text = re.sub(r\"\\b[Cc]amerounais(e)?\\b\", \"[NATIONALITY]\", text)\n",
        "    text = re.sub(r\"\\b[Ff]ran√ßais(e)?\\b\", \"[NATIONALITY]\", text)\n",
        "    return text\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/quotes_output.csv\")\n",
        "data[\"Quote\"] = data[\"Quote\"].apply(replace_country_nationality)\n",
        "output_path = '/content/drive/MyDrive/Thesis/data/UPDATED_quotes_output.csv'\n",
        "\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Tokenized quotes saved to {output_path}\")\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/cameroun_quotes_output.csv\")\n",
        "data[\"Quote\"] = data[\"Quote\"].apply(replace_country_nationality)\n",
        "\n",
        "output_path = '/content/drive/MyDrive/Thesis/data/UPDATED_cameroun_quotes_output.csv'\n",
        "\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Tokenized quotes saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vKBkAEZAzsK"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt60Q-g0rvDz",
        "outputId": "fb48ea6f-cf24-4311-9375-0f24f794285d"
      },
      "outputs": [],
      "source": [
        "nltk.download('all', download_dir='/usr/local/share/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1v20lsOA2fj",
        "outputId": "3bad0942-ddd9-42cf-c3cc-c596a6fb9e1b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/UPDATED_cameroun_quotes_output.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df['tokenized_quote'] = df['Quote'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# output_path = '/content/drive/MyDrive/Thesis/data/cameroun_quotes_tokenized.csv'\n",
        "output_path = '/content/drive/MyDrive/Thesis/data/UPDATED_cameroun_quotes_tokenized.csv'\n",
        "\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Tokenized quotes saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rry6G7czBkZi"
      },
      "source": [
        "## Le Parisien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSGDKsb9Bnwv",
        "outputId": "a5b021f8-c5af-491f-926c-229e42793fee"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data if not already available\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/UPDATED_quotes_output.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Tokenize the quotes in the 'quote' column\n",
        "df['tokenized_quote'] = df['Quote'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "# output_path = '/content/drive/MyDrive/Thesis/data/cameroun_quotes_tokenized.csv'\n",
        "output_path = '/content/drive/MyDrive/Thesis/data/UPDATED_quotes_tokenized.csv'\n",
        "\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Tokenized quotes saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC0Xz5RqHGkR"
      },
      "source": [
        "# Statisitics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpGXujCCNboj"
      },
      "source": [
        "## Standard French Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3l09WzIHI0K",
        "outputId": "4499dd1e-8f95-4e7b-fbf4-823dd03ad88f"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/UPDATED_quotes_tokenized.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df['word_count'] = df['tokenized_quote'].apply(lambda x: len(eval(x)))\n",
        "total_word_count = df['word_count'].sum()\n",
        "\n",
        "# Flatten all tokens and filter out stopwords and non-alphabetic tokens\n",
        "all_words = [word.lower() for tokens in df['tokenized_quote'].apply(eval) for word in tokens if word.isalpha()]\n",
        "filtered_words = [word for word in all_words if word not in stop_words]\n",
        "\n",
        "top_words = Counter(filtered_words).most_common(10)\n",
        "average_word_count = df['word_count'].mean()\n",
        "unique_words = len(set(filtered_words))\n",
        "\n",
        "print(\"Total Word Count:\", total_word_count)\n",
        "print(\"Average Word Count per Quote:\", average_word_count)\n",
        "print(\"Top 10 Most Common Words:\", top_words)\n",
        "print(\"Unique Word Count:\", unique_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0FlgtN3Ng4M"
      },
      "source": [
        "## Cameroonian French Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjmEtGoSHtog",
        "outputId": "3b2806b9-0b42-46ca-b727-d01dbf7d52fc"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "# Load the tokenized data\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/UPDATED_cameroun_quotes_tokenized.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Word count for each quote\n",
        "df['word_count'] = df['tokenized_quote'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "# Total word count\n",
        "total_word_count = df['word_count'].sum()\n",
        "\n",
        "# Flatten all tokens and filter out stopwords and non-alphabetic tokens\n",
        "all_words = [word.lower() for tokens in df['tokenized_quote'].apply(eval) for word in tokens if word.isalpha()]\n",
        "filtered_words = [word for word in all_words if word not in stop_words]\n",
        "\n",
        "# Top 10 most common words\n",
        "top_words = Counter(filtered_words).most_common(10)\n",
        "\n",
        "# Average word count per quote\n",
        "average_word_count = df['word_count'].mean()\n",
        "\n",
        "unique_words = len(set(filtered_words))\n",
        "\n",
        "print(\"Total Word Count:\", total_word_count)\n",
        "print(\"Average Word Count per Quote:\", average_word_count)\n",
        "print(\"Top 10 Most Common Words:\", top_words)\n",
        "print(\"Unique Word Count:\", unique_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
