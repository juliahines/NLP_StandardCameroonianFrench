{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIOEEQ8J6JlP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmqFjcD66aF3",
        "outputId": "3a08e903-35b5-45e1-b707-c2f0087b5b9b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSk370VY5HRJ"
      },
      "outputs": [],
      "source": [
        "## Cameroonian French\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/UPDATED_cameroun_quotes_tokenized.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "data['tokenized_quote'] = data['tokenized_quote'].apply(literal_eval)\n",
        "data['lemmatized_quote'] = data['tokenized_quote'].apply(lemmatize_tokens)\n",
        "\n",
        "output_file = '/content/drive/MyDrive/Thesis/data/UPDATED_cameroun_quotes_lemmatized_french.csv'\n",
        "data.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qna9O3en7Xne"
      },
      "outputs": [],
      "source": [
        "## Standard French\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "file_path = '/content/drive/MyDrive/Thesis/data/UPDATED_quotes_tokenized.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "data['tokenized_quote'] = data['tokenized_quote'].apply(literal_eval)\n",
        "data['lemmatized_quote'] = data['tokenized_quote'].apply(lemmatize_tokens)\n",
        "\n",
        "output_file = '/content/drive/MyDrive/Thesis/data/UPDATED_parisien_quotes_lemmatized_french.csv'\n",
        "data.to_csv(output_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
